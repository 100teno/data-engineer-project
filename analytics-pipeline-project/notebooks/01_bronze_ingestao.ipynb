{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abdc0b11-6304-43cd-aca0-0d35dc24f2dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading the database"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/Volumes/workspace/default/data-volume/1970-2021_DISASTERS.xlsx - emdat data.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b72e26c-105e-484f-8869-06e2b6bad3dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking table structure and data"
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(10))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Colunas que devem ser observadas: Dis No, Seq, ISO, Associated Dis, Associated Dis2, Dis Mag Value, Dis Mag Scale, Reconstruction Costs ('000 US$), Insured Damages ('000 US$),\n",
    "# #Total Damages ('000 US$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d158133-eba2-4d1e-97a9-c91c5489ad5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Adjusting column names with spaces and symbols"
    }
   },
   "outputs": [],
   "source": [
    "def clean_column(col_name):\n",
    "    col_name = col_name.lower()                                 #transforma em minusculas\n",
    "    col_name = col_name.replace(\" \", \"_\").replace(\"-\", \"_\")     #remove espaços e traços por underscore                                    \n",
    "    col_name = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", col_name)         # troca caracteres problemáticos por _\n",
    "    col_name = re.sub(r\"[^0-9a-zA-Z_]\", \"\", col_name)           # remove qualquer outro caractere especial\n",
    "    col_name = re.sub(r\"_+\", \"_\", col_name)                     # colapsa underscores duplos\n",
    "    col_name = col_name.strip(\"_\")                              # remove _ no início/fim\n",
    "    \n",
    "    return col_name\n",
    "\n",
    "#aplica a função em todas as colunas do dataframe, percorrendo com o loop for\n",
    "df = df.toDF(*[clean_column(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c44a8e8-babf-4ebc-8302-5585f98ab743",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Saving df as Delta and partitioning"
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"bronze_disasters\")\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8d9bc90f-5a0b-43d1-9539-6fce00fb4735",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking if it worked and the DataFrame turned into Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DESCRIBE DETAIL `bronze_disasters`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ea6dda-e582-4c0c-ab70-64b7fe53b4d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading Bronze Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "df_disasters = spark.table(\"bronze_disasters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6954aaf4-446d-4237-8b05-3af57cca3822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checando a lista de colunas do tipo string e atribuindo à uma variável de lista.\n",
    "string_columns = [f.name for f in df_disasters.schema.fields if f.dataType.typeName == 'string']\n",
    "\n",
    "df_normalizado = df_disasters\n",
    "\n",
    "# Criando um novo DataFrame e aplicando as transformações de lower() e trim() nos campos de string\n",
    "for coluna in string_columns:\n",
    "    df_normalizado = df_normalizado.withColumn(\n",
    "        coluna,\n",
    "        lower(trim(col(coluna)))\n",
    "    )\n",
    "\n",
    "# Removendo caracteres invisíveis \n",
    "for coluna in string_columns:\n",
    "    df_normalizado = df_normalizado.withColumn(\n",
    "        coluna,\n",
    "        regexp_replace(col(coluna), r'[\\u0000-\\u001F\\u007F-\\u009F]','')\n",
    "    )\n",
    "\n",
    "# Exibindo o schema para validar as transformações\n",
    "df_normalizado.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1860093a-67fc-4291-b486-306c17ba3d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6045802507714944,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
