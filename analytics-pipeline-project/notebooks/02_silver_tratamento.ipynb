{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a6e0bd-37d9-48d4-a1c7-d43da6183468",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading a df with bronze_disasters delta table data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, concat_ws, to_date\n",
    "\n",
    "df_disasters = spark.table(\"analytics.bronze.bronze_disasters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa740174-e040-4784-a5a9-5641ba49ef37",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754938148509}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Transforming data with functions and conversion"
    }
   },
   "outputs": [],
   "source": [
    "# Checando a lista de colunas do tipo string e atribuindo à uma variável de lista.\n",
    "string_columns = [f.name for f in df_disasters.schema.fields if f.dataType.typeName == 'string']\n",
    "\n",
    "df_normalizado = df_disasters\n",
    "\n",
    "# Criando um novo DataFrame e aplicando as transformações de lower() e trim() nos campos de string\n",
    "for coluna in string_columns:\n",
    "    df_normalizado = df_normalizado.withColumn(\n",
    "        coluna,\n",
    "        lower(trim(col(coluna)))\n",
    "    )\n",
    "\n",
    "# Removendo caracteres invisíveis \n",
    "for coluna in string_columns:\n",
    "    df_normalizado = df_normalizado.withColumn(\n",
    "        coluna,\n",
    "        regexp_replace(col(coluna), r'[\\u0000-\\u001F\\u007F-\\u009F]','')\n",
    "    )\n",
    "\n",
    "# Exibindo o schema para validar as transformações\n",
    "#df_normalizado.printSchema()\n",
    "df_normalizado.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f1916c-a6df-47b8-9502-c605e2df8c75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting the columns with correct types, and creating start_date / end_date"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Lista de valores que indicam nulo\n",
    "NULL_VALUES = [\"null\", \"n/a\", \"na\", \"unknow\", \"unknown\", \"\"]\n",
    "\n",
    "# Função para converter colunas booleanas\n",
    "def to_boolean(col_name):\n",
    "    return (\n",
    "        F.when(F.col(col_name).isin(\"yes\", \"y\", \"1\", \"true\"), True)\n",
    "         .when(F.col(col_name).isin(\"no\", \"n\", \"0\", \"false\"), False)\n",
    "         .otherwise(None)\n",
    "    )\n",
    "\n",
    "# Função para limpar coordenadas (latitude/longitude)\n",
    "def clean_coordinate(col_name, negative_pattern):\n",
    "    return F.when(F.col(col_name).isNotNull(),\n",
    "                  F.when(F.col(col_name).rlike(negative_pattern),\n",
    "                         -F.expr(f\"try_cast(regexp_replace({col_name}, '[^0-9\\\\.\\\\-]', '') as double)\"))\n",
    "                   .otherwise(F.expr(f\"try_cast(regexp_replace({col_name}, '[^0-9\\\\.\\\\-]', '') as double)\"))\n",
    "           ).otherwise(None)\n",
    "\n",
    "df_silver = (\n",
    "    df_normalizado\n",
    "    # Substitui valores que indicam nulo pelo padrão do Spark (None)\n",
    "    .replace(NULL_VALUES, None)\n",
    "\n",
    "    # Conversões booleanas\n",
    "    .withColumn(\"ofda_response\", to_boolean(\"ofda_response\"))\n",
    "    .withColumn(\"appeal\", to_boolean(\"appeal\"))\n",
    "    .withColumn(\"declaration\", to_boolean(\"declaration\"))\n",
    "\n",
    "    # Conversões numéricas simples\n",
    "    .withColumn(\"year\", F.col(\"year\").cast(\"int\"))\n",
    "    .withColumn(\"start_year\", F.col(\"start_year\").cast(\"int\"))\n",
    "    .withColumn(\"start_month\", F.col(\"start_month\").cast(\"int\"))\n",
    "    .withColumn(\"start_day\", F.col(\"start_day\").cast(\"int\"))\n",
    "    .withColumn(\"end_year\", F.col(\"end_year\").cast(\"int\"))\n",
    "    .withColumn(\"end_month\", F.col(\"end_month\").cast(\"int\"))\n",
    "    .withColumn(\"end_day\", F.col(\"end_day\").cast(\"int\"))\n",
    "\n",
    "    # Coordenadas geográficas\n",
    "    .withColumn(\"latitude_clean\", clean_coordinate(\"latitude\", \"(?i)[Ss]\"))\n",
    "    .withColumn(\"longitude_clean\", clean_coordinate(\"longitude\", \"(?i)[Ww]\"))\n",
    "\n",
    "    # Outros campos numéricos\n",
    "    .withColumn(\"total_deaths\", F.col(\"total_deaths\").cast(\"int\"))\n",
    "    .withColumn(\"no_injured\", F.col(\"no_injured\").cast(\"int\"))\n",
    "    .withColumn(\"no_affected\", F.col(\"no_affected\").cast(\"int\"))\n",
    "    .withColumn(\"no_homeless\", F.col(\"no_homeless\").cast(\"int\"))\n",
    "    .withColumn(\"total_affected\", F.col(\"total_affected\").cast(\"int\"))\n",
    "    .withColumn(\"reconstruction_costs_000_us\", F.col(\"reconstruction_costs_000_us\").cast(\"double\"))\n",
    "    .withColumn(\"insured_damages_000_us\", F.col(\"insured_damages_000_us\").cast(\"double\"))\n",
    "    .withColumn(\"total_damages_000_us\", F.col(\"total_damages_000_us\").cast(\"double\"))\n",
    "\n",
    "    # Datas com tratamento de erros\n",
    "    .withColumn(\n",
    "        \"start_date\",\n",
    "        F.expr(\"try_to_timestamp(concat_ws('-', start_year, start_month, start_day))\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"end_date\",\n",
    "        F.expr(\"try_to_timestamp(concat_ws('-', end_year, end_month, end_day))\")\n",
    "    )\n",
    "\n",
    "    # Padroniza país e região\n",
    "    .withColumn(\"country\", F.trim(F.initcap(F.col(\"country\"))))\n",
    "    .withColumn(\"region\", F.trim(F.initcap(F.col(\"region\"))))\n",
    "\n",
    "    # Substitui latitude/longitude originais\n",
    "    .drop(\"latitude\", \"longitude\")\n",
    "    .withColumnRenamed(\"latitude_clean\", \"latitude\")\n",
    "    .withColumnRenamed(\"longitude_clean\", \"longitude\")\n",
    ")\n",
    "\n",
    "df_silver.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9859600a-0668-484e-8ddb-a0cd6669446f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading the silver delta table"
    }
   },
   "outputs": [],
   "source": [
    "df_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"analytics.silver.silver_disasters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5627085c-60da-4890-a58a-30cd6d2ade81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validating the data from silver delta"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# carregando a silver em um df\n",
    "df_silver = spark.table(\"analytics.silver.silver_disasters\")\n",
    "\n",
    "print(\"Schema da tabela\")\n",
    "df_silver.printSchema()\n",
    "print(f\"Total de registros: {df_silver.count()}\")\n",
    "\n",
    "# ------------------------\n",
    "# 1. Validação nulos\n",
    "# ------------------------\n",
    "print(\"\\n Nulos por colunas:\")\n",
    "df_silver.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_silver.columns]).show(truncate=False)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. Datas invalidas\n",
    "# ------------------------\n",
    "datas_invalidas = df_silver.filter(\n",
    "    (F.col(\"start_date\") < F.lit(\"1900-01-01\")) |\n",
    "    (F.col(\"start_date\") > F.current_date()) |\n",
    "    (F.col(\"end_date\") < F.lit(\"1900-01-01\")) |\n",
    "    (F.col(\"end_date\") > F.current_date())\n",
    ")\n",
    "print(f\"⏳ Linhas com datas inválidas: {datas_invalidas.count()}\")\n",
    "\n",
    "# --------------------------\n",
    "# 3️ Datas invertidas\n",
    "# --------------------------\n",
    "datas_invertidas = df_silver.filter(F.col(\"end_date\") < F.col(\"start_date\"))\n",
    "print(f\"Linhas com datas invertidas: {datas_invertidas.count()}\")\n",
    "\n",
    "# --------------------------\n",
    "# 4️ Coordenadas inválidas\n",
    "# --------------------------\n",
    "coords_invalidas = df_silver.filter(\n",
    "    (F.col(\"latitude\") < -90) | (F.col(\"latitude\") > 90) |\n",
    "    (F.col(\"longitude\") < -180) | (F.col(\"longitude\") > 180)\n",
    ")\n",
    "print(f\"Linhas com coordenadas inválidas: {coords_invalidas.count()}\")\n",
    "\n",
    "# --------------------------\n",
    "# 5 Relatório consolidado\n",
    "# --------------------------\n",
    "df_validacao = df_silver.withColumn(\n",
    "    \"erro_data_invalida\",\n",
    "    (F.col(\"start_date\") < F.lit(\"1900-01-01\")) | (F.col(\"start_date\") > F.current_date()) |\n",
    "    (F.col(\"end_date\") < F.lit(\"1900-01-01\")) | (F.col(\"end_date\") > F.current_date())\n",
    ").withColumn(\n",
    "    \"erro_coords\",\n",
    "    (F.col(\"latitude\") < -90) | (F.col(\"latitude\") > 90) |\n",
    "    (F.col(\"longitude\") < -180) | (F.col(\"longitude\") > 180)\n",
    ").withColumn(\n",
    "    \"erro_data_invertida\",\n",
    "    F.col(\"end_date\") < F.col(\"start_date\")\n",
    ")\n",
    "\n",
    "df_erros = df_validacao.filter(\n",
    "    F.col(\"erro_data_invalida\") | F.col(\"erro_coords\") | F.col(\"erro_data_invertida\")\n",
    ")\n",
    "\n",
    "print(f\"\\n Registros com algum erro: {df_erros.count()}\")\n",
    "df_erros.display(50, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_tratamento",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
